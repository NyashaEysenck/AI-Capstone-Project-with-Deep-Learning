{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHbnU6kYJaR9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoU7JDdf2CmK"
      },
      "source": [
        "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n",
        "\n",
        "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKnc0KF-2CmN"
      },
      "source": [
        "## Objective\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXGs4Bty2CmO"
      },
      "source": [
        "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHOeCsUa2CmO"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<font size = 3>\n",
        "    \n",
        "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
        "2. <a href=\"#item32\">Download Data</a>  \n",
        "3. <a href=\"#item33\">Define Global Constants</a>  \n",
        "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
        "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
        "\n",
        "</font>\n",
        "    \n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm5bWJBx2CmO"
      },
      "source": [
        "   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDv_a8-c2CmP"
      },
      "source": [
        "<a id='item31'></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kp7Mva82CmP"
      },
      "source": [
        "## Import Libraries and Packages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbv5Kw0s2CmP"
      },
      "source": [
        "Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR9vW_eT2CmQ"
      },
      "source": [
        "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkYkve0cV47W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MyRAAtMT2CmR"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4JfFkkF2CmR"
      },
      "source": [
        "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FQRw7DPn2CmR"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfnD3SDE2CmR"
      },
      "source": [
        "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LEHvP-xK2CmR"
      },
      "outputs": [],
      "source": [
        "from keras.applications import ResNet50\n",
        "from keras.applications.resnet50 import preprocess_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmMOmkl62CmR"
      },
      "source": [
        "<a id='item32'></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDCdz-Rm2CmR"
      },
      "source": [
        "## Download Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CM0b6l-2CmS"
      },
      "source": [
        "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8jlLPpVXA1m",
        "outputId": "45841299-f013-430e-d14c-7a365c4187ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "import zipfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLG8Xp1YW-uu"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# URL of the file you want to download\n",
        "url = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip\"\n",
        "\n",
        "# Path in Google Drive where you want to save the file\n",
        "file_path = \"/content/drive/My Drive/concrete_data_week3.zip\"\n",
        "\n",
        "# Download the file and save it to Google Drive\n",
        "response = requests.get(url)\n",
        "with open(file_path, \"wb\") as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "print(f\"File downloaded to {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKi3WN0k2CmS",
        "outputId": "f0e9f4ec-47f2-4a0b-ed88-f1a63f8e8860"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File unzipped to /content/drive/My Drive/concrete_data_week3/\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Path to the zip file in Google Drive\n",
        "zip_file_path = \"/content/drive/My Drive/Copy of concrete_data_week3.zip\"\n",
        "# Directory where you want to extract the contents\n",
        "extract_dir = \"/content/drive/My Drive/concrete_data_week3/\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(extract_dir):\n",
        "    os.makedirs(extract_dir)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(f\"File unzipped to {extract_dir}\")\n",
        "\n",
        "# ## get the data\n",
        "# await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip\", overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIrTQ6XD2CmS"
      },
      "source": [
        "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrhQp57c2CmT"
      },
      "source": [
        "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoSR575j2CmT"
      },
      "source": [
        "## Define Global Constants\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFPe_v8J2CmT"
      },
      "source": [
        "Here, we will define constants that we will be using throughout the rest of the lab.\n",
        "\n",
        "1. We are obviously dealing with two classes, so *num_classes* is 2.\n",
        "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
        "3. We will training and validating the model using batches of 100 images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LOF1NVC92CmT"
      },
      "outputs": [],
      "source": [
        "num_classes = 2\n",
        "\n",
        "image_resize = 224\n",
        "\n",
        "batch_size_training = 100\n",
        "batch_size_validation = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qqSf8y02CmT"
      },
      "source": [
        "<a id='item34'></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6J70cid2CmT"
      },
      "source": [
        "## Construct ImageDataGenerator Instances\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyggetdA2CmT"
      },
      "source": [
        "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wPv8b7No2CmU"
      },
      "outputs": [],
      "source": [
        "data_generator = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaKtWwkn2CmU"
      },
      "source": [
        "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3zOFfET2CmU",
        "outputId": "fcc6cbeb-e8ed-4e7d-bd26-3293246dbb62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 10001 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_generator = data_generator.flow_from_directory(\n",
        "    '/content/drive/My Drive/concrete_data_week3/concrete_data_week3/train',\n",
        "    target_size=(image_resize, image_resize),\n",
        "    batch_size=batch_size_training,\n",
        "    class_mode='categorical')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovjsErHC2CmU"
      },
      "source": [
        "**Note**: in this lab, we will be using the full data-set of 40,000 images for training and validation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-XoIbOr2CmU"
      },
      "source": [
        "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx7gqtPf2CmU",
        "outputId": "587a4879-7995-46bd-a6e7-dbba7055ea45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5001 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "## Type your answer here\n",
        "\n",
        "validation_generator = data_generator.flow_from_directory(\n",
        "    '/content/drive/My Drive/concrete_data_week3/concrete_data_week3/valid',\n",
        "    target_size=(image_resize, image_resize),\n",
        "    batch_size=batch_size_validation,\n",
        "    class_mode='categorical')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99A7cZrD2CmV"
      },
      "source": [
        "Double-click __here__ for the solution.\n",
        "<!-- The correct answer is:\n",
        "validation_generator = data_generator.flow_from_directory(\n",
        "    'concrete_data_week3/valid',\n",
        "    target_size=(image_resize, image_resize),\n",
        "    batch_size=batch_size_validation,\n",
        "    class_mode='categorical')\n",
        "-->\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUhkVwaT2CmV"
      },
      "source": [
        "<a id='item35'></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRyG3K942CmV"
      },
      "source": [
        "## Build, Compile and Fit Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjzE4hbB2CmV"
      },
      "source": [
        "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2mjeIEpg2CmV"
      },
      "outputs": [],
      "source": [
        "resnet_model = Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-2-Dbqg2Cma"
      },
      "source": [
        "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z80bJOa2Cma",
        "outputId": "cf00ed8d-3f44-459a-85b1-2a573cbc2d56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "resnet_model.add(ResNet50(\n",
        "    include_top=False,\n",
        "    pooling='avg',\n",
        "    weights='imagenet',\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CtFeIGM2Cma"
      },
      "source": [
        "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kbSc865o2Cma"
      },
      "outputs": [],
      "source": [
        "resnet_model.add(Dense(num_classes, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJyQyxuP2Cma"
      },
      "source": [
        "You can access the model's layers using the *layers* attribute of our model object.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ft3wew7p2Cmb",
        "outputId": "07600418-7c94-4612-d8b9-a17329d06694"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<keras.src.engine.functional.Functional at 0x7ce382f72980>,\n",
              " <keras.src.layers.core.dense.Dense at 0x7ce37015e440>]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resnet_model.layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMkF6nEO2Cmb"
      },
      "source": [
        "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n8Jf-Ca2Cmb"
      },
      "source": [
        "You can access the ResNet50 layers by running the following:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et0zK-yY2Cmb",
        "outputId": "da9ac122-e659-4527-cc01-0cbed0922b5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<keras.src.engine.input_layer.InputLayer at 0x7ce3913a9060>,\n",
              " <keras.src.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x7ce393ddffa0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce393dde5f0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce3913aa0b0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce3913aac80>,\n",
              " <keras.src.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x7ce3913ab3d0>,\n",
              " <keras.src.layers.pooling.max_pooling2d.MaxPooling2D at 0x7ce390d5c370>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce3913ab880>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390d5d480>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390d5fd90>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390d84370>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390d5ea10>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390d86230>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390d5d0f0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390d84460>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390d5d180>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390d86800>,\n",
              " <keras.src.layers.merging.add.Add at 0x7ce390d877f0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390d85a80>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce3913aa530>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce3913ab1c0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390d86290>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390d953c0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390d95ae0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390d97370>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390d97820>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390d94580>,\n",
              " <keras.src.layers.merging.add.Add at 0x7ce390d97280>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390da5a20>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390da4a00>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390da63e0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390da7e50>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390da5d80>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390da65c0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390db9c30>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390db89a0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390dba410>,\n",
              " <keras.src.layers.merging.add.Add at 0x7ce390dbb400>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390db9660>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390dda0b0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390dda0e0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390dd9d50>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390ddb430>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390df54b0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390df6230>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390dbb250>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390df66e0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390dd8d90>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390df6e00>,\n",
              " <keras.src.layers.merging.add.Add at 0x7ce390df76a0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390df6380>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390df57b0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390db97e0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390da5630>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390dd9240>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390df6350>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390dd94e0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390100bb0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390101c90>,\n",
              " <keras.src.layers.merging.add.Add at 0x7ce390102f50>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390103a60>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390102980>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390101c60>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390111c30>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce3901120e0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390112800>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390113880>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390111b70>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390112470>,\n",
              " <keras.src.layers.merging.add.Add at 0x7ce390113d90>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390122770>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390120190>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390123130>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390121900>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390138c70>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390138e20>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390139840>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390138490>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce39013bfa0>,\n",
              " <keras.src.layers.merging.add.Add at 0x7ce3901385b0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390150a60>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390152e00>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390152c50>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce3901223e0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390120160>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390111e10>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390dda5c0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390151ae0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390153010>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390151f90>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390103970>,\n",
              " <keras.src.layers.merging.add.Add at 0x7ce390151f00>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce39016a560>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce3901695d0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce39016b640>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce39016a860>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390174af0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390174c70>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390175630>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390174580>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce390176f80>,\n",
              " <keras.src.layers.merging.add.Add at 0x7ce390177fa0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390188880>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce39018a320>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce3901896f0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce39018b040>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce390189c60>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce39018bbb0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce3901a43a0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce3901a58a0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce3901a5a20>,\n",
              " <keras.src.layers.merging.add.Add at 0x7ce3901a6560>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce3901a7910>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce3901a5990>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce3901c8340>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce3901a6530>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce3901ca050>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce3901c83d0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce3901cba00>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce3901cbeb0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce3901a5390>,\n",
              " <keras.src.layers.merging.add.Add at 0x7ce390176530>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390189f30>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce39016a260>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce39016a9e0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce39016a140>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce3901d9000>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce3901d9180>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce3901d91e0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce3901d8a60>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce3901db130>,\n",
              " <keras.src.layers.merging.add.Add at 0x7ce3901da260>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce3901e0640>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce3901e1e40>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce3901e1f60>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce3901e3130>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce3901e35e0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce3901e2260>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce3901e39d0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce382f09420>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce382f09bd0>,\n",
              " <keras.src.layers.merging.add.Add at 0x7ce382f0aa10>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce382f0bdc0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce382f21c00>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce382f21bd0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce3901236d0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce382f22e60>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce382f22bf0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce382f237c0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce382f09a80>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce382f38610>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce382f09d80>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce382f3a6b0>,\n",
              " <keras.src.layers.merging.add.Add at 0x7ce382f20910>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce382f217b0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce382f08910>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce3901da290>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce390110e20>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce3901d8b50>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce3901762c0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce3901e2020>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce382f3ba60>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce382f39fc0>,\n",
              " <keras.src.layers.merging.add.Add at 0x7ce3901da740>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce382f3b400>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce382f584c0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce382f3b640>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce382f592a0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce382f59450>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce382f5b940>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce382f5b1f0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x7ce382f59780>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7ce382f5b670>,\n",
              " <keras.src.layers.merging.add.Add at 0x7ce382f597e0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7ce382f71570>,\n",
              " <keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D at 0x7ce382f586a0>]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resnet_model.layers[0].layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAvbqknu2Cmb"
      },
      "source": [
        "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Vz2qmubW2Cmb"
      },
      "outputs": [],
      "source": [
        "resnet_model.layers[0].trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKw-PuJX2Cmb"
      },
      "source": [
        "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbgH79_l2Cmb",
        "outputId": "7edf01b4-52ae-4bca-9573-4c760e9c5cc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50 (Functional)       (None, 2048)              23587712  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23591810 (90.00 MB)\n",
            "Trainable params: 4098 (16.01 KB)\n",
            "Non-trainable params: 23587712 (89.98 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "resnet_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zONMmIEc2Cmb"
      },
      "source": [
        "Next we compile our model using the **adam** optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Cv95oH982Cmc"
      },
      "outputs": [],
      "source": [
        "resnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDjvAC5s2Cmc"
      },
      "source": [
        "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "pf-eXFzo2Cmc"
      },
      "outputs": [],
      "source": [
        "steps_per_epoch_training = len(train_generator)\n",
        "steps_per_epoch_validation = len(validation_generator)\n",
        "num_epochs = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJtRF1vH2Cmd",
        "outputId": "04826f63-9172-427a-c3fa-ea8c29545e6b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "101"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "steps_per_epoch_training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ya2wDtM2Cmd"
      },
      "source": [
        "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh9r352t2Cmd",
        "outputId": "ccb73f1e-6c0c-4f31-e577-5c0454930606"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-27-d93f702eb440>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  fit_history = resnet_model.fit_generator(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "101/101 [==============================] - 121s 969ms/step - loss: 0.0598 - accuracy: 0.9768 - val_loss: 0.0138 - val_accuracy: 0.9952\n",
            "Epoch 2/2\n",
            "101/101 [==============================] - 92s 910ms/step - loss: 0.0071 - accuracy: 0.9987 - val_loss: 0.0094 - val_accuracy: 0.9968\n"
          ]
        }
      ],
      "source": [
        "fit_history = resnet_model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch_training,\n",
        "    epochs=num_epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=steps_per_epoch_validation,\n",
        "    verbose=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AN0qQ472Cme"
      },
      "source": [
        "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "OCtaALOa2Cme"
      },
      "outputs": [],
      "source": [
        "# drive.mount('/content/drive')\n",
        "resnet_model_save_path = '/content/drive/My Drive/classifier_resnet_model.keras'\n",
        "resnet_model.save(resnet_model_save_path)\n",
        "resnet_model_save_path = '/content/drive/My Drive/classifier_resnet_model.h5'\n",
        "resnet_model.save(resnet_model_save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOcEmkTyGxit"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlfVhHLf2Cme"
      },
      "source": [
        "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4R1Zv-l2Cme"
      },
      "source": [
        "### Thank you for completing this lab!\n",
        "\n",
        "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOB_-nrC2Cmf"
      },
      "source": [
        "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6oxGzK42Cmf"
      },
      "source": [
        "\n",
        "## Change Log\n",
        "\n",
        "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
        "|---|---|---|---|\n",
        "| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
        "| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di02ad3H2Cmf"
      },
      "source": [
        "<hr>\n",
        "\n",
        "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
